{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5HfueMetTGhF",
    "outputId": "a0b2c907-094d-47f4-91f6-5122441dde39",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip -q install transformers sentence-transformers trafilatura tqdm regex\n",
    "\n",
    "import os, re, json, time, math, requests, concurrent.futures\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, util as st_util\n",
    "import trafilatura\n",
    "\n",
    "# Fast, deterministic-ish\n",
    "import random, numpy as np\n",
    "def set_seed(s=42):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(s)\n",
    "set_seed(42)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "AMP_DTYPE = (torch.bfloat16 if (DEVICE.type==\"cuda\" and torch.cuda.is_bf16_supported()) else torch.float16)\n",
    "print(\"Device:\", DEVICE, \"AMP:\", AMP_DTYPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mTSM5mCTSr5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# <<< SET THESE IF YOU HAVE THEM >>>\n",
    "SERPAPI_KEY = os.getenv(\"SERPAPI_KEY\", \"\")      # https://serpapi.com/\n",
    "TAVILY_KEY  = os.getenv(\"TAVILY_API_KEY\", \"\")   # https://tavily.com/\n",
    "VOYAGE_KEY  = os.getenv(\"VOYAGE_API_KEY\", \"\")   # if you use Voyage\n",
    "\n",
    "# Source preferences (keeps quality high)\n",
    "WHITELIST = [\n",
    "    # your desired news channels domain\n",
    "]\n",
    "\n",
    "# Retrieval & ranking knobs (good defaults)\n",
    "MAX_RESULTS = 20\n",
    "BI_MIN_COS  = 0.35\n",
    "TOP_K_AFTER_RERANK = 6\n",
    "\n",
    "# NLI thresholds (balanced)\n",
    "ENTAIL_THR = 0.65\n",
    "CONTRA_THR = 0.55\n",
    "\n",
    "# Timeouts\n",
    "HTTP_TIMEOUT = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rj_MG0HrThiq",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def search_serpapi(q: str, n=MAX_RESULTS) -> List[Dict[str, Any]]:\n",
    "    if not SERPAPI_KEY: return []\n",
    "    url = \"https://serpapi.com/search.json\"\n",
    "    params = {\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": q,\n",
    "        \"num\": min(n, 20),\n",
    "        \"tbm\": \"nws\",  # Google News\n",
    "        \"api_key\": SERPAPI_KEY\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=HTTP_TIMEOUT); r.raise_for_status()\n",
    "        data = r.json()\n",
    "        items = []\n",
    "        for it in data.get(\"news_results\", []):\n",
    "            link = it.get(\"link\") or \"\"\n",
    "            if not link: continue\n",
    "            items.append({\n",
    "                \"source\": it.get(\"source\",\"\"),\n",
    "                \"title\": it.get(\"title\",\"\"),\n",
    "                \"snippet\": it.get(\"snippet\",\"\"),\n",
    "                \"date\": it.get(\"date\",\"\"),\n",
    "                \"url\": link\n",
    "            })\n",
    "        return items\n",
    "    except Exception as e:\n",
    "        print(\"SerpAPI error:\", e)\n",
    "        return []\n",
    "\n",
    "def search_tavily(q: str, n=MAX_RESULTS) -> List[Dict[str, Any]]:\n",
    "    if not TAVILY_KEY: return []\n",
    "    try:\n",
    "        r = requests.post(\"https://api.tavily.com/search\",\n",
    "            json={\"api_key\": TAVILY_KEY, \"query\": q, \"max_results\": n, \"search_depth\":\"basic\"},\n",
    "            timeout=HTTP_TIMEOUT)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        items = []\n",
    "        for it in data.get(\"results\", []):\n",
    "            items.append({\n",
    "                \"source\": it.get(\"source\",\"\"),\n",
    "                \"title\": it.get(\"title\",\"\"),\n",
    "                \"snippet\": it.get(\"content\",\"\")[:300],\n",
    "                \"date\": it.get(\"published_date\",\"\"),\n",
    "                \"url\": it.get(\"url\",\"\")\n",
    "            })\n",
    "        return items\n",
    "    except Exception as e:\n",
    "        print(\"Tavily error:\", e)\n",
    "        return []\n",
    "\n",
    "def combined_search(q: str) -> List[Dict[str, Any]]:\n",
    "    # Merge, whitelist, dedup by URL host+title\n",
    "    items = search_serpapi(q) + search_tavily(q)\n",
    "    def host(u):\n",
    "        m = re.search(r\"https?://([^/]+)/\", u+\"/\")\n",
    "        return m.group(1) if m else \"\"\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for it in items:\n",
    "        h = host(it.get(\"url\",\"\"))\n",
    "        if WHITELIST and h and not any(w in h for w in WHITELIST):\n",
    "            continue\n",
    "        key = (h, it.get(\"title\",\"\").strip().lower())\n",
    "        if key in seen: continue\n",
    "        seen.add(key); out.append(it)\n",
    "    return out[:MAX_RESULTS]\n",
    "\n",
    "def fetch_html(url: str) -> str:\n",
    "    try:\n",
    "        r = requests.get(url, timeout=HTTP_TIMEOUT, headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
    "        r.raise_for_status()\n",
    "        return r.text\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_text(url: str) -> str:\n",
    "    html = fetch_html(url)\n",
    "    if not html: return \"\"\n",
    "    txt = trafilatura.extract(html, include_comments=False, include_tables=False) or \"\"\n",
    "    # light cleanup\n",
    "    return re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "\n",
    "def enrich_with_body(items: List[Dict[str,Any]]) -> List[Dict[str,Any]]:\n",
    "    # Parallel fetch bodies for speed\n",
    "    def job(it):\n",
    "        body = extract_text(it[\"url\"])\n",
    "        it2 = dict(it); it2[\"body\"] = body\n",
    "        return it2\n",
    "    out = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as ex:\n",
    "        for it2 in tqdm(ex.map(job, items), total=len(items), desc=\"Downloading articles\"):\n",
    "            out.append(it2)\n",
    "    # drop empties\n",
    "    return [x for x in out if x.get(\"body\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nM5htJy8TjL9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Bi-encoder (fast cosine)\n",
    "bi = SentenceTransformer(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\", device=\"cuda\" if DEVICE.type==\"cuda\" else \"cpu\")\n",
    "\n",
    "# Cross-encoder (pairwise rerank)\n",
    "ce_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "ce_tok  = AutoTokenizer.from_pretrained(ce_name)\n",
    "ce      = AutoModelForSequenceClassification.from_pretrained(ce_name).to(DEVICE).eval()\n",
    "\n",
    "\n",
    "def select_top_candidates(claim: str, docs: List[Dict[str,Any]], k=TOP_K_AFTER_RERANK) -> List[Dict[str,Any]]:\n",
    "    if not docs: return []\n",
    "    # Compose a short “evidence text” per doc: title + 2–3 sentences from body\n",
    "    def lead(body: str, sent_n=3):\n",
    "        sents = re.split(r'(?<=[.!?])\\s+', body.strip())\n",
    "        return \" \".join(sents[:sent_n])\n",
    "    cand_texts = []\n",
    "    for d in docs:\n",
    "        t = (d.get(\"title\",\"\") or d.get(\"headline\",\"\")).strip()\n",
    "        b = lead(d.get(\"body\",\"\"))\n",
    "        text = (t + \". \" + b).strip()\n",
    "        cand_texts.append(text if text else t)\n",
    "\n",
    "    # 1) Bi-encoder cosine filter\n",
    "    q = bi.encode([claim], convert_to_tensor=True, normalize_embeddings=True)\n",
    "    em = bi.encode(cand_texts, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    sims = st_util.cos_sim(q, em).cpu().numpy().ravel()\n",
    "    kept = [(i, sims[i]) for i in range(len(docs)) if sims[i] >= BI_MIN_COS]\n",
    "    if not kept:\n",
    "        # keep top-6 anyway to avoid empty\n",
    "        kept = sorted([(i, sims[i]) for i in range(len(docs))], key=lambda x: -x[1])[:min(6, len(docs))]\n",
    "\n",
    "    # 2) Cross-encoder rerank\n",
    "    kept_idx = [i for i,_ in kept]\n",
    "    pairs = [(claim, cand_texts[i]) for i in kept_idx]\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for s in range(0, len(pairs), 16):\n",
    "            a,b = zip(*pairs[s:s+16])\n",
    "            enc = ce_tok(list(a), list(b), truncation=True, max_length=384, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
    "            logits = ce(**enc).logits.squeeze(-1).detach().cpu().numpy().tolist()\n",
    "            scores.extend(logits)\n",
    "    order = sorted(range(len(kept_idx)), key=lambda j: -scores[j])\n",
    "    top = [docs[kept_idx[j]] for j in order[:k]]\n",
    "    for j in range(len(top)):\n",
    "        top[j][\"rerank_score\"] = float(scores[order[j]])\n",
    "    return top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PmB7YT-XTnhj",
    "outputId": "cfa173d6-86bc-4a48-8e73-011241a46b5e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NLI_NAME = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"\n",
    "\n",
    "nli_tok  = AutoTokenizer.from_pretrained(NLI_NAME)\n",
    "nli      = AutoModelForSequenceClassification.from_pretrained(NLI_NAME).to(DEVICE).eval()\n",
    "print(\"NLI labels:\", nli.config.id2label)  # should be {0: 'CONTRADICTION', 1: 'NEUTRAL', 2: 'ENTAILMENT'}\n",
    "# Map by label *names*, not indices\n",
    "lbl_map = {v.lower(): int(k) for k, v in nli.config.id2label.items()}\n",
    "\n",
    "def _get_idx(name_opts):\n",
    "    for name in name_opts:\n",
    "        name = name.lower()\n",
    "        if name in lbl_map:\n",
    "            return lbl_map[name]\n",
    "    # fallback: assume order [C, N, E]\n",
    "    return {\"C\":0, \"N\":1, \"E\":2}\n",
    "\n",
    "C_IDX = _get_idx([\"contradiction\", \"label_0\", \"contradict\", \"c\"])\n",
    "N_IDX = _get_idx([\"neutral\", \"label_1\", \"n\"])\n",
    "E_IDX = _get_idx([\"entailment\", \"label_2\", \"entails\", \"e\"])\n",
    "\n",
    "print(\"Resolved label indices ->\", {\"C\":C_IDX, \"N\":N_IDX, \"E\":E_IDX})\n",
    "\n",
    "def yield_chunks(text: str, max_prem_toks=420):\n",
    "    toks = nli_tok.encode(text, add_special_tokens=False)\n",
    "    for i in range(0, len(toks), max_prem_toks):\n",
    "        chunk_ids = toks[i:i+max_prem_toks]\n",
    "        yield nli_tok.decode(chunk_ids, skip_special_tokens=True)\n",
    "\n",
    "@torch.no_grad()\n",
    "def nli_best_chunk(premise_text: str, claim: str) -> Tuple[float,float,float,str]:\n",
    "    best = (-1.0, -1.0, -1.0, \"\")\n",
    "    for ch in yield_chunks(premise_text):\n",
    "        enc = nli_tok(ch, claim, truncation=True, max_length=512, padding=False, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.autocast(device_type=DEVICE.type, dtype=AMP_DTYPE) if DEVICE.type==\"cuda\" else torch.no_grad():\n",
    "            out = nli(**enc)\n",
    "        probs = out.logits.softmax(-1).squeeze().detach().cpu().numpy().tolist()  # [C,N,E]\n",
    "        C,N,E = probs\n",
    "        if E > best[2]:\n",
    "            best = (C,N,E,ch)\n",
    "    return best  # (C, N, E, chunk)\n",
    "\n",
    "def verify_claim(claim: str, docs: List[Dict[str,Any]]) -> Dict[str,Any]:\n",
    "    # Build premise as: title + first 3–4 sentences + first long paragraph\n",
    "    results = []\n",
    "    for d in docs:\n",
    "        title = (d.get(\"title\",\"\") or d.get(\"headline\",\"\")).strip()\n",
    "        body  = d.get(\"body\",\"\")\n",
    "        # compose premise\n",
    "        sents = re.split(r'(?<=[.!?])\\s+', body.strip())\n",
    "        premise = (title + \". \" + \" \".join(sents[:4]) + \" \" + (\" \".join(sents[4:10]) if len(sents)>6 else \"\")).strip()\n",
    "        C,N,E,ch = nli_best_chunk(premise, claim)\n",
    "        results.append({\"doc\": d, \"C\": C, \"N\": N, \"E\": E, \"chunk\": ch})\n",
    "\n",
    "    if not results:\n",
    "        return {\"label\":\"NO_EVIDENCE\", \"reason\":\"no documents\"}\n",
    "\n",
    "    best_ent = max(results, key=lambda r: r[\"E\"])\n",
    "    best_con = max(results, key=lambda r: r[\"C\"])\n",
    "\n",
    "    if best_ent[\"E\"] >= ENTAIL_THR:\n",
    "        return {\"label\":\"SUPPORTED\", \"confidence\": float(best_ent[\"E\"]), \"evidence\": best_ent}\n",
    "    if best_con[\"C\"] >= CONTRA_THR:\n",
    "        return {\"label\":\"REFUTED\", \"confidence\": float(best_con[\"C\"]), \"evidence\": best_con}\n",
    "\n",
    "    return {\"label\":\"NO_EVIDENCE\", \"reason\":\"all neutral or low-confidence\",\n",
    "            \"best_entail\": float(best_ent[\"E\"]), \"best_contra\": float(best_con[\"C\"])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFWVXaQRUFaA",
    "outputId": "a7542241-6379-4e95-8a5d-6134acb3d1fc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Cell 5 — NLI (robust label mapping + safe chunking) =====\n",
    "from typing import Tuple\n",
    "import torch, re\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "NLI_NAME = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"  # ok\n",
    "\n",
    "nli_tok  = AutoTokenizer.from_pretrained(NLI_NAME)\n",
    "nli      = AutoModelForSequenceClassification.from_pretrained(NLI_NAME).to(DEVICE).eval()\n",
    "print(\"NLI labels:\", nli.config.id2label)\n",
    "\n",
    "# 1) Map labels by NAME, not by index (robust across models)\n",
    "# Map labels by NAME, not index\n",
    "lbl_map = {v.lower(): int(k) for k, v in nli.config.id2label.items()}\n",
    "\n",
    "def idx_for(name_opts, default_idx):\n",
    "    for name in name_opts:\n",
    "        name = name.lower()\n",
    "        if name in lbl_map:\n",
    "            return lbl_map[name]\n",
    "    return default_idx  # fallback to common order [C,N,E] -> 0,1,2\n",
    "\n",
    "C_IDX = idx_for([\"contradiction\", \"label_0\", \"contradict\", \"c\"], 0)\n",
    "N_IDX = idx_for([\"neutral\", \"label_1\", \"n\"], 1)\n",
    "E_IDX = idx_for([\"entailment\", \"label_2\", \"entails\", \"e\"], 2)\n",
    "\n",
    "print(\"Resolved label indices ->\", {\"C\": C_IDX, \"N\": N_IDX, \"E\": E_IDX})\n",
    "\n",
    "\n",
    "# print(\"Resolved label indices ->\", {\"C\":C_IDX, \"N\":N_IDX, \"E\":E_IDX})\n",
    "\n",
    "# 2) Dynamic chunking that leaves room for the claim; truncate ONLY the premise\n",
    "def yield_chunks_dynamic(premise_text: str, claim: str, max_total=512, safety=12):\n",
    "    hyp_ids = nli_tok.encode(claim, add_special_tokens=False)\n",
    "    max_prem = max(64, max_total - len(hyp_ids) - safety - 3)  # ~3 specials\n",
    "    prem_ids = nli_tok.encode(premise_text, add_special_tokens=False)\n",
    "    for i in range(0, len(prem_ids), max_prem):\n",
    "        yield nli_tok.decode(prem_ids[i:i+max_prem], skip_special_tokens=True)\n",
    "\n",
    "@torch.no_grad()\n",
    "def nli_best_chunk(premise_text: str, claim: str) -> Tuple[float,float,float,str]:\n",
    "    best = (-1.0, -1.0, -1.0, \"\")\n",
    "    for ch in yield_chunks_dynamic(premise_text, claim, max_total=512, safety=12):\n",
    "        enc = nli_tok(\n",
    "            ch, claim,\n",
    "            truncation=\"only_first\",   # truncate premise only\n",
    "            max_length=512,\n",
    "            padding=False,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(DEVICE)\n",
    "        # Use AMP on GPU, but always cast logits to float32 before numpy()\n",
    "        ctx = torch.autocast(device_type=DEVICE.type, dtype=AMP_DTYPE) if DEVICE.type==\"cuda\" else torch.no_grad()\n",
    "        with ctx:\n",
    "            out = nli(**enc)\n",
    "        logits = out.logits.float()                      # avoid bf16/fp16 numpy issues\n",
    "        probs  = torch.softmax(logits, dim=-1).squeeze(0)\n",
    "        C = float(probs[C_IDX]); N = float(probs[N_IDX]); E = float(probs[E_IDX])\n",
    "        if E > best[2]:\n",
    "            best = (C, N, E, ch)\n",
    "    return best  # (C, N, E, best_chunk_text)\n",
    "\n",
    "def verify_claim(claim: str, docs: List[Dict[str,Any]]) -> Dict[str,Any]:\n",
    "    # Build premise as: title + first 3–4 sentences + a bit more context\n",
    "    results = []\n",
    "    for d in docs:\n",
    "        title = (d.get(\"title\",\"\") or d.get(\"headline\",\"\")).strip()\n",
    "        body  = d.get(\"body\",\"\")\n",
    "        sents = re.split(r'(?<=[.!?])\\s+', body.strip())\n",
    "        premise = (title + \". \" + \" \".join(sents[:4]) + \" \" + (\" \".join(sents[4:10]) if len(sents)>6 else \"\")).strip()\n",
    "        C,N,E,ch = nli_best_chunk(premise, claim)\n",
    "        results.append({\"doc\": d, \"C\": C, \"N\": N, \"E\": E, \"chunk\": ch})\n",
    "\n",
    "    if not results:\n",
    "        return {\"label\":\"NO_EVIDENCE\", \"reason\":\"no documents\"}\n",
    "\n",
    "    best_ent = max(results, key=lambda r: r[\"E\"])\n",
    "    best_con = max(results, key=lambda r: r[\"C\"])\n",
    "\n",
    "    if best_ent[\"E\"] >= ENTAIL_THR:\n",
    "        return {\"label\":\"SUPPORTED\", \"confidence\": float(best_ent[\"E\"]), \"evidence\": best_ent}\n",
    "    if best_con[\"C\"] >= CONTRA_THR:\n",
    "        return {\"label\":\"REFUTED\", \"confidence\": float(best_con[\"C\"]), \"evidence\": best_con}\n",
    "\n",
    "    return {\n",
    "        \"label\":\"NO_EVIDENCE\",\n",
    "        \"reason\":\"all neutral or low-confidence\",\n",
    "        \"best_entail\": float(best_ent[\"E\"]),\n",
    "        \"best_contra\": float(best_con[\"C\"])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FC1d-5yLXpAH",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fact_check(claim: str) -> Dict[str,Any]:\n",
    "    print(f\"\\n=== CLAIM ===\\n{claim}\\n\")\n",
    "\n",
    "    # 1. Retrieve headlines/snippets\n",
    "    hits = combined_search(claim)   # from Cell 3\n",
    "    if not hits:\n",
    "        return {\"label\":\"NO_EVIDENCE\", \"reason\":\"search returned 0 items\"}\n",
    "\n",
    "    # 2. Fetch full article bodies\n",
    "    docs = enrich_with_body(hits)   # from Cell 3\n",
    "    if not docs:\n",
    "        return {\"label\":\"NO_EVIDENCE\", \"reason\":\"no article text extracted\"}\n",
    "\n",
    "    # 3. Rerank/filter candidates\n",
    "    top_docs = select_top_candidates(claim, docs)   # from Cell 4\n",
    "\n",
    "    # 4. Verify claim with NLI\n",
    "    out = verify_claim(claim, top_docs)   # from Cell 5\n",
    "\n",
    "    # 5. Pretty print evidence\n",
    "    label = out[\"label\"]\n",
    "    print(f\"\\nRESULT: {label}\")\n",
    "    if label in (\"SUPPORTED\",\"REFUTED\"):\n",
    "        ev = out[\"evidence\"]; d = ev[\"doc\"]\n",
    "        print(f\"Confidence: {out['confidence']:.3f}\")\n",
    "        print(f\"Source: {d.get('source','?')}\\nTitle: {d.get('title','')}\\nURL: {d.get('url','')}\")\n",
    "        print(\"\\n--- Evidence excerpt ---\")\n",
    "        print((ev[\"chunk\"] or \"\").strip()[:800])\n",
    "    else:\n",
    "        print(\"Reason:\", out.get(\"reason\"))\n",
    "        print(f\"Best entail={out.get('best_entail',0):.3f} | Best contra={out.get('best_contra',0):.3f}\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rutAyu5SXrud",
    "outputId": "c006e07b-e695-4531-da58-59c190271819",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result = fact_check(\"\")\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
